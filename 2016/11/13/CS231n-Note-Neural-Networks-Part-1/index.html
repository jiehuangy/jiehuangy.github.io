<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.2" />






<meta property="og:type" content="article">
<meta property="og:title" content="[CS231n Note] Neural Networks Part 1">
<meta property="og:url" content="http://yoursite.com/2016/11/13/CS231n-Note-Neural-Networks-Part-1/index.html">
<meta property="og:site_name" content="EverMind">
<meta property="og:image" content="http://yoursite.com/images/Note-CS231n-Neural-Networks-Part-1-f1.png">
<meta property="og:updated_time" content="2016-11-13T13:18:38.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="[CS231n Note] Neural Networks Part 1">
<meta name="twitter:image" content="http://yoursite.com/images/Note-CS231n-Neural-Networks-Part-1-f1.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"hide"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/2016/11/13/CS231n-Note-Neural-Networks-Part-1/"/>


  <title> [CS231n Note] Neural Networks Part 1 | EverMind </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="//schema.org/WebPage" lang="en">

  










  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="//schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">EverMind</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Find Something Interesting.</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule" rel="section">
            
            Schedule
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                [CS231n Note] Neural Networks Part 1
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-11-13T20:11:02+08:00" content="2016-11-13">
              2016-11-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Neural-Networks/" itemprop="url" rel="index">
                    <span itemprop="name">Neural Networks</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="/images/Note-CS231n-Neural-Networks-Part-1-f1.png" alt=""><br><a id="more"></a></p>
<h2 id="Commonly-used-activation-functions"><a href="#Commonly-used-activation-functions" class="headerlink" title="Commonly used activation functions"></a>Commonly used activation functions</h2><p><strong>Sigmoid</strong>. It has two major drawbacks:</p>
<ul>
<li>Sigmoids saturate and kill gradients. when the neuron‚Äôs activation saturates at either tail of 0 or 1, the gradient at these regions is almost zero. If the local gradient is very small, it will effectively ‚Äúkill‚Äù the gradient and almost no signal will flow through the neuron to its weights and recursively to its data.</li>
<li>Sigmoid outputs are not zero-centered. If the data coming into a neuron is always positive (e.g. $x&gt;0$ elementwise in $f=w^Tx+b$)), then the gradient on the weights $w$ will during backpropagation become either all be positive, or all negative. This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights.</li>
</ul>
<p><strong>Tanh</strong>. Like the sigmoid neuron, its activations saturate, but unlike the sigmoid neuron its output is zero-centered.</p>
<p><strong>ReLU</strong>. The Rectified Linear Unit has become very popular in the last few years.</p>
<ul>
<li>(+) It was found to greatly accelerate the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. </li>
<li>(+) The ReLU can be implemented by simply thresholding a matrix of activations at zero.</li>
<li>(-) ReLU units can ‚Äúdie‚Äù. If the unit is inactive in forward pass, then its local gradient is zero in backward pass. As a result, the unit will forever be zero.</li>
</ul>
<p><strong>Leaky ReLU</strong>. Leaky ReLUs are one attempt to fix the ‚Äúdying ReLU‚Äù problem. Instead of the function being zero when $x &lt; 0$, a leaky ReLU will instead have a small negative slope. That is, the function computes $f(x)=ùüô(x<0)(Œ±x)+ùüô(x>=0)(x)$.</0)(Œ±x)+ùüô(x></p>
<p><strong>Maxout</strong>. The Maxout neuron computes the function $max(w^{T_1}x+b1,w^{T_2}x+b2)$. Notice that both ReLU and Leaky ReLU are a special case of this form.</p>
<p><strong>TLDR</strong>. ‚ÄúWhat neuron type should I use?‚Äù Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of ‚Äúdead‚Äù units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.</p>
<h2 id="Neural-Network-architectures"><a href="#Neural-Network-architectures" class="headerlink" title="Neural Network architectures"></a>Neural Network architectures</h2><p><strong>Naming conventions</strong>. Notice that when we say N-layer neural network, we do not count the input layer. </p>
<p><strong>Output layer</strong>. Unlike all layers in a Neural Network, the output layer neurons most commonly do not have an activation function. This is because the last output layer is usually taken to represent the class scores.</p>
<h2 id="Representational-power"><a href="#Representational-power" class="headerlink" title="Representational power"></a>Representational power</h2><p>It turns out that Neural Networks with at least one hidden layer are universal approximators. In other words, the neural network can approximate any continuous function. The fact that deeper networks (with multiple hidden layers) can work better than a single-hidden-layer networks is an empirical observation, despite the fact that their representational power is equal.</p>
<h2 id="Setting-number-of-layers-and-their-sizes"><a href="#Setting-number-of-layers-and-their-sizes" class="headerlink" title="Setting number of layers and their sizes"></a>Setting number of layers and their sizes</h2><p>In practice, it is always better to use these methods to control overfitting instead of the number of neurons. There are many ways to prevent overfitting in Neural Networks (such as L2 regularization, dropout, input noise). </p>
<p>The subtle reason behind this is that smaller networks are harder to train with local methods such as Gradient Descent: It‚Äôs clear that their loss functions have relatively few local minima, but it turns out that many of these minima are easier to converge to, and that they are bad (i.e. with high loss). Conversely, bigger neural networks contain significantly more local minima, but these minima turn out to be much better in terms of their actual loss. </p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/11/13/CS231n-Note-Backpropagation-Intuitions/" rel="next" title="[CS231n Note] Backpropagation, Intuitions">
                <i class="fa fa-chevron-left"></i> [CS231n Note] Backpropagation, Intuitions
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="//schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="Huang Jie" />
          <p class="site-author-name" itemprop="name">Huang Jie</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">3</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Commonly-used-activation-functions"><span class="nav-number">1.</span> <span class="nav-text">Commonly used activation functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Network-architectures"><span class="nav-number">2.</span> <span class="nav-text">Neural Network architectures</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Representational-power"><span class="nav-number">3.</span> <span class="nav-text">Representational power</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Setting-number-of-layers-and-their-sizes"><span class="nav-number">4.</span> <span class="nav-text">Setting number of layers and their sizes</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Huang Jie</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.2"></script>



  



  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  

  

  


</body>
</html>
